---
layout:     post
date:       2020-03-04
tag:        note
author:     BY Zhi-kai Yang
---
>统计推断-3
>
>Mar.04, 2020,  Zhengzhou, Henan
>
>内容：随机样本的性质

这一章正式开始研究样本。

## 随机样本

**定义：** 如果随机变量$X_1,X_2,..., X_n$ 相互独立且有相同的边缘密度（质量）函数$f(x)$, 则称$X_1,...,X_n$是**总体**$f(x)$的大小为$n$的随机样本。或称$X_1,...,X_n$是独立同分布随机变量(iid.).
$$
f(x_1,...,x_n|\theta) = \prod_{i=1}^nf(x_i|\theta)
$$
在统计学问题中，如果假定我们所观测的总体服从某给定参数组中的某一个分布，而参数值真值位置，则取自该总体的随机样本的联合概率密度函数形如上式。通过考察$\theta$ 的不同取值，我们可以讨论不同总体下随机样本的行为。

注意上述定义描述的抽样模型是 无限总体的抽样； 有限总体上的无放回抽样有时也称作简单随机抽样。

## 随机样本中随机变量的和

设$X_1,...,X_n$ 是从总体中抽取的大小为$n$的随机样本，$T(x_1,...,x_n)$是定义在$(X_1,...,X_n)$的样本空间上的实值或向量值函数，则随机变量或随机向量$Y=T(X_1,...,X_n)$成为一个统计量（statistic）, $Y$的概率分布称为$Y$的抽样分布。

下面是三个常用的统计量：

- 样本均值 sample mean:
  $$
  \bar{X}=\frac{X_{1}+\cdots+X_{n}}{n}=\frac{1}{n} \sum_{i=1}^{n} X_{i}
  $$
  

- 样本方差 sample variance:
  $$
  S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2
  $$

- 样本标准差 std： $S = \sqrt{S^2}$

**定理：**

- $$
  \min_a \sum_{i=1}^n (x_i-a)^2 = \sum_{i=1}^n(x-\bar{x})^2
  $$

- $$
  (n-1)s^2 = \sum_{i=1}^nx_i^2 -n\bar{x}
  $$

现在我们通过考察某些统计量的期望 讨论抽样分布，下面的结论非常有用：

**引理：** 设$X_1,...,X_n$ 是从总体中抽取的随机样本，函数$g(x)$ 使得$Eg(X_1)$ 和 $Varg(X_1)$都存在，则：
$$
E(\sum_{i=1}^ng(X_i)) =nEg(X_1) \\
Var(\sum_{i=1}^ng(X_i)) = nVarg(X_1)
$$
**定理** 设随机样本$X_1,...,X_n$ 取自期望为$\mu$, 方差为$\sigma^2 < +\infty$ 的总体, 则

- $E\bar{X} = \mu$;
- $Var\bar{X} = \frac{\sigma^2}{n}$
- $ES^2 = \sigma^2$

注意上述定理表述是一般情形，要求总体的方差有限。

使用之前的引理其实很好证明上述三个定理：

>Proof:
>$$
>E\bar{X} = E(1/n \sum_iX_i ) = 1/n E(\sum_iX_i) = \frac{1}{n}nEX_1 = \mu
>$$
>
>$$
>Var\bar{X} = Var(\frac{1}{n}\sum_iX_i) = \frac{1}{n^2}Var(\sum_iX_i) = \frac{1}{n^2}nVarX_1 = \frac{\sigma^2}{n}
>$$
>
>$$
>\begin{aligned} \mathrm{ES}^{2} &=\mathrm{E}\left(\frac{1}{n-1}\left[\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}\right]\right) \\ &=\frac{1}{n-1}\left(n \mathrm{E} X_{1}^{2}-n \mathrm{E} \bar{X}^{2}\right) \\ &=\frac{1}{n-1}\left(n\left(\sigma^{2}+\mu^{2}\right)-n\left(\frac{\sigma^{2}}{n}+\mu^{2}\right)\right)=\sigma^{2} \end{aligned}
>$$

上面三个定理为统计量与总体的参数 建立了联系。统计量$\bar{X}$, $S^2$ 分别是$\mu$, $\sigma^2$的 无偏估计。

下面讨论一下$\bar{X}$的抽样分布。

我们可以用变量替换法求$Y=X_1+,...,+X_n$ 和$\bar{X}$ 的概率密度函数。下面的**卷积公式**将非常有用：

$Z = X+Y$ 的概率密度函数为：
$$
f_Z（z)= \int_{-\infty}^{+\infty} f_X(w)f_Y(z-w)dw
$$

> 卷积公式的证明需要扩展一个较为一般的定理：
>
> 假如随机变量$u=g_1(x,y), v = g_2(x,y)$, 是一种一对一映射，我们很容易解出逆变换$x=h_1(u,v), y=h_2(u,v)$ 则  
> $$
> J=\left|\begin{array}{ll}\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}\end{array}\right|=\frac{\partial x}{\partial u} \frac{\partial y}{\partial v}-\frac{\partial y \partial x}{\partial u} \frac{\partial x}{\partial v}
> $$
> 则$U,V$的 联合分布为：
> $$
> f_{U,V} (u,v )= f_{X,Y}(h_1(u,v), h_2(u,v))|J|
> $$
> 利用这个结论我们证明卷积公式：
>
> $(X,Y) \rightarrow (X+Y, X) = (Z,W)$
>
> 则$Z,W$的联合分布为：
> $$
> f_{Z,W}(z,w) = f_{X,Y}(w, z-w)|J|,\ |J| =1
> $$
> 积分得：
> $$
> f_Z(z) = \int f_X(w)f_Y(z-w)dw
> $$

卷积公式是由随机变量求和运算导出的，除此之外，我们还可以为随即变量的差、积以及商推导类似的公式。

如果进行抽样的总体服从位置-尺度分布族或某种指数分布族，则随机变量和， 特别是$\bar{X}$的抽样分布很容易求得. 下面分别讨论这两种情形以结束本节：

如果随机样本$X_1,...,X_n$ 取自参数为$(\mu, \sigma)$的Cauchy分布总体，则$\bar{X}$ 也服从参数为$(\mu, \sigma)$的Cauchy分布。注意此时$\bar{X}$分布的离散度仅由$\sigma$确定，与样本大小$n$ 无关，这与一般情形的定理（总体存在方差有限时）形成了鲜明的对比，那里已知$Var\bar{X} = \frac{\sigma^2}{n}$, 他随样本增大而减小。

指数族分布有如下定理

**定理** 指数族分布
$$
f(x | \theta)=h(x) c(\theta) \exp \left(\sum_{i=1}^{k} w_{i}(\theta) t_{i}(x)\right)
$$
定义统计量$T_1,...,T_k$为
$$
T_i(X_1,...,X_n) = \sum_{j=1}^nt_i(X_j)
$$
则$(T_1,...,T_k)$ 的分布是如下形式的指数族分布
$$
f_{T}\left(u_{1}, \cdots, u_{k} | \theta\right)=H\left(u_{1}, \cdots, u_{k}\right)[c(\theta)]^{n} \exp \left(\sum_{i=1}^{k} w_{i}(\theta) u_{i}\right)
$$

## 正态分布的抽样

### 样本方差与样本均值的性质

**定理** 设随机样本$X_1,...,X_n$ 取自服从$N(\mu, \sigma^2)$ 分布的总体，$\bar{X} = (1/n)\sum_iX_i$ 且 $S^2=1/(n-1)\sum_i(X_i-\bar{X})^2$ ,则：

- $\bar{X}$ 和$S^2$ 是独立随机变量；
- $\bar{X}$服从 $N(\mu, \sigma^2/n)$的分布；
- $(n-1)S^2/\sigma^2$ 服从自由度为$n-1$的$\chi^2$分布

**引理** 关于卡方随机变量的一些事实

如果$Z$是$n(0,1)$随机变量，则$Z^2 \sim \chi_1^2$, 即标准正态随机变量的平方是$\chi^2$随机变量。

如果$X_1,...,X_n$独立且$X_i \sim \chi_{p_i}^2$，则$X_1+,...,+X_n \sim \chi_{p_1+...+p_i}^2$, 即独立的$\chi^2$随机变量之和仍为$\chi^2$随机变量，且其自由度为原随机变量自由度之和。

### 导出分布：t分布与F分布

student t分布的由来

如果随机样本$X_1,...,X_n$取自服从$N(\mu, \sigma^2)$分布d 总体，则随机变量
$$
\frac{\bar{X} - \mu}{\sigma/\sqrt{n}}
$$
服从$n(0,1)$分布，在$\sigma$已知时通过此分布就可以很好地对$\mu$进行推断，但大多情况$\sigma$未知，自然的想法是研究$\frac{\bar{X} - \mu}{S/\sqrt{n}}$的分布。

他的分布经过一些变换可以很明显地看出：
$$
\frac{\bar{X}-\mu}{S / \sqrt{n}}=\frac{(\bar{X}-\mu) /(\sigma / \sqrt{n})}{\sqrt{S^{2} / \sigma^{2}}}
$$
上式简化为$U/\sqrt{V/p}$ 的分布，其中$U\sim N(0,1)$, $V \sim \chi_p^2$ ,且 $U,V $是独立分布。这个分布就是student t 分布。

**定义** 设随机样本$X_1,...,X_n$取自服从$n(\mu, \sigma^2)$分布的总体，则称量$\frac{(X-\bar{X})}{S/\sqrt{n}}$ 服从自由度为$n-1$的t分布。t分布如下：
$$
f_{T}(t)=\frac{\Gamma\left(\frac{p+1}{2}\right)}{\Gamma\left(\frac{p}{2}\right)} \frac{1}{(p \pi)^{1 / 2}} \frac{1}{\left(1+t^{2} / p\right)^{(p+1) / 2}}, \quad-\infty<t<+\infty
$$
称$T$服从自由度为$p$的t分布，记做$T \sim t_p$

另一种导出分布是F分布，他与t分布的动机不同，是作为方差比值分布而自然产生的。

**定义** 设随机样本$X_1,...,X-n$取自$N(\mu_X, \sigma_X^2)$的总体，$Y_1,...,Y_m$取自$N(\mu_Y, \sigma_Y^2)$的总体，且与$X_1,...,X_n$独立。则随机变量$\frac{S_X^2/\sigma_X^2}{S_Y^2/\sigma_Y^2}$服从自由度为$n-1$和$m-1$的F分布。他的pdf为：
$$
f_{F}(x)=\frac{\Gamma\left(\frac{p+q}{2}\right)}{\Gamma\left(\frac{p}{2}\right) \Gamma\left(\frac{q}{2}\right)}\left(\frac{p}{q}\right)^{1 / 2} \frac{x^{(p / 2)-1}}{[1+(p / q) x]^{(p+q) / 2}}, \quad 0<x<+\infty
$$
$F$服从自由度为$p,q$的F分布。

实际上F分布可以看做两个卡方分布之比，$(U/p)/(V/q)$其中 $U\sim \chi_p^2, V\sim \chi_q^2$

关于F分布的一些事实：

- 如果$X \sim F_{p,q}$, 则$\frac{1}{X} \sim F_{q,p}$, 即F随机变量的倒数仍是$F$随机变量；
- 如果$X \sim t_q$ , 则$X^2 \sim F_{1,q}$
- 如果$X \sim F_{p,q}$ 则 $\frac{(p/q)X}{(1+(p/q)X)}$服从参数为$(p/2, q/2)$的贝塔分布。

## 收敛的概念

本节允许样本大小到达无穷，并着重考察某些样本量在这种情况下的行为。我们将详细讨论三类收敛

### 依概率收敛

**定义**： 称随机变量序列$X_1,...,X_n,...$  依概率收敛与随机变量$X$, 如果对任意$\epsilon >0$ 都有
$$
\lim_{n \rightarrow \infty} P(|X_n-X|\ge \epsilon) = 0
$$
统计学家通常很关心样本均值序列收敛于常数的情形，譬如下述著名的定理：

**弱大数定律**  设$X_1,X_2,....$ 是一列独立同分布的随机变量，且$EX_i = \mu, VarX_i =\sigma^2 < \infty$, 令 $\bar{X_n} = (1/n)\sum_{i=1}^nX_i$, 则对任意的$\epsilon >0$ 都有 $\lim_{n \rightarrow \infty} P(|\bar{X_n}-\mu| < \epsilon) = 1$,  即$\bar{X_n}$依概率收敛到$\mu$

弱大数定律表明当n趋于无穷时 “同一” 样本量的序列收敛于常数，这个性质成为`相合性`

**定理** 设随机变量序列$X_1,X_2,...,$ 依概率收敛于随机变量$X$, $h$是一个连续函数，则$h(X_1),h(X_2),...$依概率收敛于$h(X)$

### 殆必收敛 Converage Almost Surely

**定义** 称随机变量序列$X_1,X_2,...$ 殆必收敛于随机变量$X$, 如果对任意$\epsilon >0$ 都有
$$
P(\lim_{n \rightarrow \infty}|X_n-X| < \epsilon) =1
$$
**强大数定律** 同样的条件，满足$P(\lim_{n \rightarrow +\infty}|\bar{X_n} -\mu| < \epsilon) = 1$

> 初次读到这里简直一脸问号，强弱大数定律和之前的两种收敛到底什么却别，极限的概率 和 概率的极限到底怎么理解？这里贴[一篇回答 ](https://stats.stackexchange.com/questions/2230/convergence-in-probability-vs-almost-sure-convergence) , 他从某个角度直观解释了不同；
>
> 下述是我自己的想法
>
> 假如令$A_n = \{x| |\bar{X_n}-X| >\epsilon\}$ , 弱大数定律表示
> $$
> P(A_n)  \rightarrow 0
> $$
> 意思是当n大道一定程度后，失败概率会收敛到0，但是仍有极小概率会发生；而强大数定律描述的收敛更强，他说随着n的增大，失败的次数会收敛到一个常数。也就是
> $$
> \sum_{n=1}^\infty I(A_n) =C
> $$
> 利用示性函数与概率间的联系$EI(A_n) = P(A_n)$ ，得：
> $$
> E(\sum I(A_n)) = \sum_{n=1}^nEI(A_n) = \sum_{n=1}^\infty P(A_n) = C
> $$
> 也就是$P(A_{n\rightarrow \infty}) = 0$

### 依分布收敛

**定义**  称随机变量序列$X_1,X_2,...$ 依分布收敛与随机变量$X$, 如果对$F_X(x)$ 的任意连续点$x$, 都有
$$
\lim_{n \rightarrow \infty} F_{X_n}(x) = F_X(x)
$$
上述定义实质上是针对累计分布函数的收敛定义。但另两种收敛也蕴含着依分布收敛。

**定理** 如果随机变量序列$X_1,X_2,...$ 依概率收敛于随机变量$X$, 则该序列也依分布收敛与$X$

**定理** 随机变量$X_1,X_2,...$ 依概率收敛与常数$\mu$ 当且仅当序列依分布收敛于$\mu$即
$$
\forall \epsilon >0,\ P(|X_n - \mu| > \epsilon) \rightarrow 0  
$$
等价于
$$
P\left(X_{n} \leqslant x\right) \rightarrow\left\{\begin{array}{l}0 & x<\mu \\ 1 & x> \mu\end{array}\right.
$$
样本均值在大样本下的行为，尤其是其及极限分布，在统计学研究中十分重要，下面著名的定理对此刻画：

**中心极限定理：** 设$X_1,X_2,...$是独立同分布的随机变量，且在0的某邻域内存在矩母函数（i.e. 存在$h >0$ 使得任意$|t|< h$ ,$M_{X_i}(t)$存在）。令$EX_i = \mu$, $VarX_i= \sigma^2 >0 $ （由矩母函数的存在性可知均值和方差均有限）， 以及$\bar{X_n} = (1/n)\sum_{i=1}^nX_i$.  设$G_n(x)$为$\sqrt{n}(\bar{X_n}-\mu)/\sigma$的累计分布函数，则对任意$x$, 都有
$$
\lim _{n \rightarrow \infty} G_{n}(x)=\int_{-\infty}^{x} \frac{1}{\sqrt{2 \pi}} \mathrm{e}^{-y^{2} / 2} \mathrm{d} y
$$


即$\sqrt{n}(\bar{X_n}-\mu)/\sigma$ 服从极限标准正态分布。

中心极限定理的直观意义是非凡的，他不需要对分布做任何假设就可任推出正态性。其关键在于正态性是由“小”（由于方差有限）且独立的扰动累加得到的，因此方差有限的假设必不可少。

## 生成随机样本

其实就是采样的方法；考虑如何根据给定的分布$f(x|\theta)$ 生成随机样本$X_1,...,X_n$

首先基于的事实是很多算法都可以生成均匀分布的伪随机数，现在关心的是均匀随机变量能否转换为其他变量。

### 直接法

如果函数$g(u)$可以表示成初等函数且当随机变量$U$服从$(0,1)$区间上的均匀分布时，变换后的随机变量$Y= g(U)$满足某指定分布，则可用直接法生成随机变量$Y$;  我们知道针对连续随机变量，任何分布都可以变换为均匀分布，该变换的逆恰是直接法的关键g

这样的变换通常建立在概率积分变换上：
$$
F_{Y}^{-1}(u)=y \leftrightarrow u=\int_{-\infty}^{y} f_{y}(t) \mathrm{d} t
$$
上式若应用于指数分布将很容易处理，然而也有很多情况不是很方便。

### 间接法

**定理** 设$Y \sim f_Y(y)$, $V \sim f_V(v)$, 其中$f_Y, f_V$有相同的支撑集且
$$
M = \sup_y \frac{f_Y(y)}{f_V(y)} < + \infty
$$
按下列步骤可以生成随机变量$Y \sim f_Y$

- 生成独立随机变量$U,V$,  其中$U$服从(0,1) 均匀分布，$V \sim f_V$
- 如果$U < \frac{1}{M} f_Y(V)/f_V(V)$ 则令$Y=V$ 否则返回上一步

