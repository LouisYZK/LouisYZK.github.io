---
layout:     post
date:       2020-03-11
tag:        note
author:     BY Zhi-kai Yang
---
>Mar 10, 2020, Still in ZhengZhou
>
>统计推断05： 假设检验

## 假设检验

**定义** 假设，hyperthesis 就是关于总体参数的一个陈述

**定义** 一个假设检验问题中的两个互补假设成为原假设 null hyperthesis 和备选假设 alternative hyperthesis. 分别把他们记做$H_0$和$H_1$

在一个假设检验问题中，试验者在观测到样本以后决定是接受$H_0$为真还是认为其为假而拒绝$H_0$, 即认为$H_1$为真。

一个假设检验过程或者说一个假设检验是一个法则，他明确描述：

- 对于哪些样本值应该决定接受$H_0$为真
- 对于哪些样本值应该拒绝$H_0$ 而接受$H_1$为真

那些由拒绝$H_0$的样本构成的样本空间的子集叫做拒绝域 rejection region. 或临界区域 critical region. 拒绝域的补集叫做接受区域 acceptance region.

## 检验的求法

### 似然比检验

**定义** 关于检验$H_{0}: \theta \in \Theta_{0}$,  对于$H_1: \theta \in \Theta_0^C$ 的似然比检验统计量是
$$
\lambda (x) = \frac{\sup_{\Theta_0} L(\theta|x)}{\sup_\Theta L(\theta|x)}
$$
任何一个拒绝区域的形似为$\{ x: \lambda(x) \le c\}$ 的检验都叫做似然比检验。likelihood ratio test LRT. 这里的$c \in [0,1]$

**定理** 设$T(X)$是关于$\theta$的一个充分统计量，而$\lambda^\star (t)$和$\lambda(x)$ 分别是依赖于$T$和$X$的LRT统计量，则对于样本空间的每一个$x$, 有$\lambda^\star (T(x)) = \lambda(x)$

### 并-交检验 与 交-并检验

设
$$
H_0: \theta \in \cap_{\gamma\in \Gamma} \Theta_\gamma
$$
则关于并-交检验的拒绝域为
$$
\bigcup_{\gamma \in \Gamma}\left\{\boldsymbol{x}: T_{\gamma}(\boldsymbol{x}) \in R_{\gamma}\right\}
$$
假如任何一个假设$H_{0\gamma}$被拒绝了，原假设$H_0$被拒绝。某些情况下，能够求得一个交-并检验的简单的表达式，特别的如果每一个个别检验都具有$\{x: T(x) >c \}$ 形式的拒绝域，这时：
$$
\bigcup_{\gamma \in \Gamma}\left\{\boldsymbol{x}: T_{\gamma}(\boldsymbol{x})>c\right\}=\left\{\boldsymbol{x} \quad: \sup _{\gamma \in \Gamma} T_{\gamma}(\boldsymbol{x})>c\right\}
$$
这样关于$H_0$的检验统计量就是$T(x)= \sup_{\gamma \in \Gamma} T_\gamma(x)$ , 在一些例子中$T(x)$有一些简单的形式，这些例子会在之后出现。

用并-交检验构造检验，在原假设被方便地表示成一个交集时是有用的。另一种方法，交-并方法，则当原假设被方便地表示成一个并集时可能是有用的：
$$
H_0 : \theta\in \bigcup_{\gamma \in \Gamma} \Theta_\gamma
$$
交-并检验的拒绝域就是：
$$
\bigcap_{\gamma \in \Gamma } \{x: T(x) \in R_\gamma \}
$$

## 检验的评价方法

### 错误概率与功效函数

如果$\theta \in \Theta_0$ 但是假设检验不正确地判定拒绝$H_0$ ,于是假设检验就犯了第一类错误。另一方面，$\theta \in \Theta_0^C$ 但是假设检验判定接受$H_0$ ，于是就犯了第二类错误。

![](https://cdn.mathpix.com/snip/images/eVGZadSY7d_9O-KTrqBso1JgOZnvOAXHtfa745EOWWo.original.fullsize.png)

**定义** 一个拒绝区域为$R$的假设检验的功效函数 power function 是由 $\beta(\theta) = P_\theta(X \in R)$ 定义的函数。

以下两个术语在讨论控制犯第一类错误的概率时是有用的：

- 设$ 0 \le  \alpha \le 1$ ,称一个功效函数$\beta(\theta)$的检验是真实水平为$\alpha$的检验size a test 如果$\sup_{\theta \in \Theta_0} \beta(\theta) = \alpha$
- 设$ 0 \le  \alpha \le 1$ ，称一个功效函数$\beta(\theta)$的检验是水平为$\alpha$ 的检验 level a test 如果$\sup_{\theta \in \Theta_0} \beta(\theta) \le \alpha$

除了水平$\alpha$ 之外，人们还可能关注检验的其他特征，例如，我们乐意使一个检验在$\theta \in \Theta_0^C$ 时更倾向于拒绝$H_0$, 这就引出所谓无偏检验：

**定义** 一个功效函数为$\beta(\theta)$ 是检验是无偏都是，如果对每一个$\theta' \in \Theta_0^C$ 和$\theta '' \in \Theta_0$ 有$\beta(\theta') \ge \beta(\theta '')$

### 最大功效检验

上节我们控制了犯第一类错误的概率至多为$\alpha$, 在这样一个类中，一个好检验犯第二类错误的概率也应当小，即当$\theta \in \Theta_0^C$ 时他的功效函数应比较大。如果一个检验犯第二类错误的概率比这类中所有其他检验更小，他理应是这个类中最有检验的强有力的竞争者。以下给出一个形式化定义：

**定义** 设$\mathcal{C}$ 是一个关于 $H_0: \theta \in \Theta_0$ 对$H_1: \theta \in \Theta _0^C$ 的检验类。 $\mathcal{C}$ 中一个功效函数为$\beta(\theta)$ 的检验是一个一致最大功效$\mathcal{C}$ 类检验，uniformly most powerful (UMP) class C test 如果对每个$\theta \in \Theta_0^C$ 与每个$\mathcal{C}$中d 功效函数$\beta ' (\theta)$ 都有$\beta(\theta) \ge \beta'(\theta)$

下面的著名定理清楚地描述了在原假设和备择假设都只含有一个关于样本的概率分布（即H0和H1都是简单假设）的情况，哪些检验是UMP水平为$\alpha$ 的检验：

**Neyman-Pearson 引理**  考虑检验$H_1: \theta =\theta_0$ 对$H_1: \theta = \theta_1$， 利用一个拒绝域为$R$的检验，$R$ 满足对某个$k \ge 0$:

- 条件1： if $f(x|\theta_1) > kf(x|\theta_0)$ 则 $x \in R$ ;  if $f(x|\theta_1) < kf(x|\theta_0)$ 则 $x \in R^C$
- 条件2： $\alpha = P_{\theta_0}(X \in R)$

则有：

- 充分性： 任意满足条件1，2的检验是一个UMP水平为$\alpha$的检验
- 必要性：如果存在一个满足条件1，2的检验， 则每一个UMP**水平为** $\alpha$的检验是 **真实水平**$\alpha$ 的检验（满足条件2）

上述定理也可推论到一个关于的$\theta$的充分统计量。

当我们导出一个满足条件1，2的检验，从而是一个UMP水平为$\alpha$ 的检验时，通常易于把不等式写成如$\frac{f(x|\theta_1)}{f(x|\theta_0)} >k$ 的形式。

特别地，断言一个一元参数过大，如$H: \theta \ge \theta_0$ 或过小 $\theta \le \theta_0$ 的假设叫做 单侧假设(one -side hypothesis) . 断言中的参数范围包含大的又包含小的参数值假设，如$H: \theta \neq \theta_0$ 叫做双侧假设 two -sided hypothesis 有很大的一类有UMP水平为$\alpha$的检验的问题牵涉到单侧假设。

**定义 MLR** 称一元随机变量具有单调似然比 monotone likelihood ratio MLR 如果对每一个$\theta_2 > \theta_1$ ， $f(x|\theta_2) / f(x|\theta_1)$ 都是$x$ 单调函数

很多普通的分布族具有MLR. 例如正态分布(方差已知，均值未知)、泊松分布和二项分布。

**Karlin-Rubin 定理** 考虑检验$H_0: \theta \le \theta_0$ 对$H_1: \theta >\theta_0$,  设$T$ 是一个关于$\theta$的充分统计量 且他的pmf\pdf 满足MLR。 则对于任何$t_0$, “当且仅当 $T>t_0$ 时拒绝$H_0$” 的检验是一个UMP水平为$\alpha$ 的检验，其中$\alpha = P_{\theta_0}(T > t_0)$ 

上述定理反之亦成立，即同样的条件，"拒绝$H_0: \theta \ge \theta_0$ 当且仅当 $T< t_0$" 的检验也是一个UMP水平为$\alpha = P(T<t_0)$ 的检验。

### p-值

**定义** p-值 p-value $p(X)$是一个满足对每一个样本点$x$ ,都有$0 \le p(x) \le 1$的检验统计量，如果$p(X)$的值小则可以作为$H_1$ 为真的证据。一个p-值成为是有效的，如果对每一个$\theta \in \Theta_0$ 和每一个$0 \le \alpha \le 1$ 都有
$$
P_\theta(p(X) \le \alpha) \le \alpha
$$
根据上式，当且仅当$p(X) \le \alpha$ 时拒绝$H_0$ 的检验就是一个水平为$\alpha$的检验。p-值越小，就越强烈地拒绝$H_0$ 。

最普通的定义一个p-值的方法由下述定理给出：

**定理**  设$W(X)$ 是这样一个检验统计量，如$W$的值大则可以作为$H_1$为真的依据，对于每个样本点$x$, 定义 
$$
p(x) = \sup_{\theta \in \Theta_0}P_\theta(W(X) \ge W(x))
$$
则$p(X)$ 是一个有效的p-值。

[文章](https://mp.weixin.qq.com/s?__biz=MjM5MDEzNDAyNQ==&mid=200652178&idx=1&sn=ebcfde94db2998f2bcf0407232d5c7c7&scene=2&from=timeline&isappinstalled=0#rd) 讲述了对p值的争议。

